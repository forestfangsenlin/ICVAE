{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b7cad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import gzip\n",
    "import h5py\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from model import MolecularICVAE\n",
    "from utils import decode_smiles_from_indexes, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1409cfc4",
   "metadata": {},
   "source": [
    "###  load data and create conditional input from labels\n",
    "\n",
    "\n",
    "\n",
    "(1) **y_train_ic (batch_size, 1,33)** is the condition input which is concatenated with the SMILES one-hot encoding vector **X_train (batch_size, 120,33)** as the input **(batch_size, 121,33)** of ICVAE.\n",
    "\n",
    "(2) **y_train_l (batch_size, 128)** is the condition input which is used to constrain the latent vector (batch_size, 120,33) to the molecular properties.\n",
    "\n",
    "note: we only set the first two dimension of latent vector as the conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60a7e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_dim = 128\n",
    "\n",
    "X_train, X_test, charset = load_dataset('./data/processed.h5')\n",
    "\n",
    "y_train = np.load(\"./prop_np/weight/y_train_norm.npy\")\n",
    "y_test = np.load(\"./prop_np/weight/y_test_norm.npy\")\n",
    "\n",
    "y_train_ic = np.repeat(y_train[:,np.newaxis], 33, -1)\n",
    "y_test_ic = np.repeat(y_test[:,np.newaxis], 33, -1)\n",
    "\n",
    "y_train_l = np.repeat(y_train[:, np.newaxis], lat_dim, axis=1)\n",
    "y_test_l = np.repeat(y_test[:, np.newaxis], lat_dim, axis=1)\n",
    "\n",
    "y_train_l[:,2:] = 0.\n",
    "y_test_l[:,2:] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2b364e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_X_train = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
    "torch_X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "\n",
    "torch_ic_train = torch.from_numpy(y_train_ic).type(torch.FloatTensor) \n",
    "torch_ic_test = torch.from_numpy(y_test_ic).type(torch.FloatTensor)\n",
    "\n",
    "torch_l_train = torch.from_numpy(y_train_l).type(torch.FloatTensor) \n",
    "torch_l_test = torch.from_numpy(y_test_l).type(torch.FloatTensor)\n",
    "\n",
    "torch_lc_train = torch.from_numpy(y_train).type(torch.FloatTensor) \n",
    "torch_lc_test = torch.from_numpy(y_test).type(torch.FloatTensor)\n",
    "\n",
    "train = torch.utils.data.TensorDataset(torch_X_train, torch_ic_train, torch_l_train, torch_lc_train)\n",
    "test = torch.utils.data.TensorDataset(torch_X_test, torch_ic_test, torch_l_test, torch_lc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1309be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=250)\n",
    "test_loader = torch.utils.data.DataLoader(test, shuffle=True, batch_size=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ead9761",
   "metadata": {},
   "source": [
    "### correlate the latent vector with molecular properties\n",
    "\n",
    "the only difference between our model with other vae-based models is that we use **(z_mean-y_arg)** to fouce the mean value of latent vector into the molecular property value, while other vae-based models only use **z_mean**.\n",
    "\n",
    "**note: y_arg is the y_train_l**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bef9b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x_decoded_mean, x, z_mean, z_logvar, y_arg):\n",
    "    xent_loss = F.binary_cross_entropy(x_decoded_mean, x, size_average=False)\n",
    "    kl_loss = -0.5 * torch.sum(1 + z_logvar - (z_mean-y_arg).pow(2) - z_logvar.exp())\n",
    "    return 0.5*xent_loss + kl_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef57f71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "epochs = 100\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = MolecularICVAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcb2ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    KL_loss = 0\n",
    "    latent_arr = []\n",
    "    label_arr = []\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        oh, ic, l, lc = data\n",
    "        oh,label_ic,label_l, label_lc = oh.unsqueeze(1).to(device),ic.to(device),l.to(device),lc.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, mean, logvar,latent = model(oh, label_ic,label_lc)\n",
    "        loss, kl_loss = vae_loss(output, oh.squeeze(1), mean, logvar, label_l)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        KL_loss+=kl_loss.item()\n",
    "        optimizer.step()\n",
    "        latent_arr.append(latent.cpu().detach().numpy())\n",
    "        label_arr.append(label_l.cpu().detach().numpy()[:,:7])\n",
    "        \n",
    "    print('train CL: '+str((train_loss-KL_loss) / len(train_loader.dataset)) + '  train KL: ' + str(KL_loss / len(train_loader.dataset)))\n",
    "    \n",
    "    return train_loss / len(train_loader.dataset),latent_arr, label_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c78a0db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    KL_loss = 0\n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        oh, ic, l, lc = data\n",
    "        oh,label_ic,label_l, label_lc = oh.unsqueeze(1).to(device),ic.to(device),l.to(device),lc.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, mean, logvar,latent = model(oh, label_ic,label_lc)\n",
    "        loss, kl_loss = vae_loss(output, oh.squeeze(1), mean, logvar, label_l)\n",
    "        KL_loss+=kl_loss.item()\n",
    "        test_loss += loss.item()\n",
    "    print('test CL: '+str((test_loss-KL_loss) / len(test_loader.dataset)) + '  test KL: ' + str(KL_loss / len(test_loader.dataset)))\n",
    "    \n",
    "    return test_loss / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e72befae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yi/anaconda3/envs/dl2/lib/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train CL: 362.369909375  train KL: 18481.088859375\n",
      "test CL: 343.8789375  test KL: 4754.1374625\n",
      "train CL: 342.2986390625  train KL: 4099.9815203125\n",
      "test CL: 341.67598125  test KL: 3650.54915625\n",
      "train CL: 341.5542484375  train KL: 3435.884471875\n",
      "test CL: 341.30831875  test KL: 3134.9903125\n",
      "train CL: 339.8165671875  train KL: 2930.931703125\n",
      "test CL: 338.0603875  test KL: 2607.9593625\n",
      "train CL: 339.5169796875  train KL: 9302.8506375\n",
      "test CL: 337.26789375  test KL: 4212.7898875\n",
      "train CL: 336.34605625  train KL: 3676.73779375\n",
      "test CL: 335.8059625  test KL: 3147.043775\n",
      "train CL: 335.875625  train KL: 2905.365784375\n",
      "test CL: 334.623925  test KL: 2576.554525\n",
      "train CL: 335.09417734375  train KL: 2372.05545625\n",
      "test CL: 333.939609375  test KL: 2120.351171875\n",
      "train CL: 333.43574453125  train KL: 2002.349496875\n",
      "test CL: 333.579584375  test KL: 1814.442915625\n",
      "train CL: 332.8581484375  train KL: 1745.85172734375\n",
      "test CL: 331.87445625  test KL: 1610.31475\n",
      "train CL: 332.35548828125  train KL: 1543.73333984375\n",
      "test CL: 334.5742484375  test KL: 1433.1627734375\n",
      "train CL: 332.4757140625  train KL: 1380.8945609375\n",
      "test CL: 331.4897703125  test KL: 1284.5770140625\n",
      "train CL: 331.881657421875  train KL: 1231.530927734375\n",
      "test CL: 331.3303515625  test KL: 1211.3192390625\n",
      "train CL: 331.589446875  train KL: 1119.3160546875\n",
      "test CL: 332.5510828125  test KL: 1067.6853609375\n",
      "train CL: 331.218240234375  train KL: 1024.924092578125\n",
      "test CL: 336.30619375  test KL: 1042.0255875\n",
      "train CL: 330.49207734375  train KL: 933.594375\n",
      "test CL: 331.9676578125  test KL: 892.2590234375\n",
      "train CL: 330.1035421875  train KL: 836.14849296875\n",
      "test CL: 329.68875  test KL: 820.307028125\n",
      "train CL: 329.529366796875  train KL: 772.3922171875\n",
      "test CL: 329.4286140625  test KL: 785.016475\n",
      "train CL: 328.95922734375  train KL: 752.60777578125\n",
      "test CL: 329.9261875  test KL: 717.975990625\n",
      "train CL: 328.7746291015625  train KL: 686.9793009765625\n",
      "test CL: 329.1131828125  test KL: 699.641121875\n",
      "train CL: 328.5442193359375  train KL: 621.0693291015625\n",
      "test CL: 327.44487421875  test KL: 587.54090390625\n",
      "train CL: 328.3745318359375  train KL: 577.3969994140625\n",
      "test CL: 335.050059375  test KL: 577.2895890625\n",
      "train CL: 327.87255078125  train KL: 523.745221875\n",
      "test CL: 327.99078125  test KL: 653.87086875\n",
      "train CL: 327.5221859375  train KL: 514.0335515625\n",
      "test CL: 334.78474921875  test KL: 513.49530390625\n",
      "train CL: 327.3146861328125  train KL: 479.2473455078125\n",
      "test CL: 328.37108203125  test KL: 497.49027109375\n",
      "train CL: 327.0363  train KL: 445.727983984375\n",
      "test CL: 326.48781328125  test KL: 444.09175703125\n",
      "train CL: 326.7687154296875  train KL: 439.5525533203125\n",
      "test CL: 326.14989375  test KL: 574.0405234375\n",
      "train CL: 326.065362890625  train KL: 475.674290234375\n",
      "test CL: 326.997871875  test KL: 417.1814359375\n",
      "train CL: 325.77376328125  train KL: 380.570038671875\n",
      "test CL: 326.39946171875  test KL: 449.11247578125\n",
      "train CL: 325.8646  train KL: 401.703905859375\n",
      "test CL: 324.52422890625  test KL: 476.20803203125\n",
      "train CL: 325.1336669921875  train KL: 363.9724255859375\n",
      "test CL: 325.82962265625  test KL: 347.08439296875\n",
      "train CL: 325.10904189453123  train KL: 343.15958505859373\n",
      "test CL: 325.958965625  test KL: 339.4700359375\n",
      "train CL: 324.6232984375  train KL: 332.08650234375\n",
      "test CL: 325.559652734375  test KL: 331.228222265625\n",
      "train CL: 324.4194893554687  train KL: 319.53565791015626\n",
      "test CL: 323.47496484375  test KL: 363.43579609375\n",
      "train CL: 323.88838857421877  train KL: 315.21276181640627\n",
      "test CL: 322.81151953125  test KL: 294.67810703125\n",
      "train CL: 324.0873208984375  train KL: 295.5732732421875\n",
      "test CL: 322.7121734375  test KL: 283.5656484375\n",
      "train CL: 324.21317275390624  train KL: 266.65088935546873\n",
      "test CL: 325.477708203125  test KL: 318.460485546875\n",
      "train CL: 323.65061455078126  train KL: 264.7345252929687\n",
      "test CL: 321.600475  test KL: 299.2954265625\n",
      "train CL: 323.24364609375  train KL: 282.257633203125\n",
      "test CL: 322.087212890625  test KL: 276.970427734375\n",
      "train CL: 323.219121484375  train KL: 250.657950390625\n",
      "test CL: 321.674949609375  test KL: 270.535792578125\n",
      "train CL: 322.6275767578125  train KL: 249.077703125\n",
      "test CL: 325.717589453125  test KL: 249.538494140625\n",
      "train CL: 322.760869921875  train KL: 225.567901171875\n",
      "test CL: 320.9868515625  test KL: 227.343334375\n",
      "train CL: 322.68585537109374  train KL: 212.88629599609374\n",
      "test CL: 323.8755375  test KL: 249.01365546875\n",
      "train CL: 322.2007771484375  train KL: 220.3841810546875\n",
      "test CL: 321.889619921875  test KL: 233.788110546875\n",
      "train CL: 322.3064672363281  train KL: 198.48135014648437\n",
      "test CL: 320.5019171875  test KL: 203.2373328125\n",
      "train CL: 321.7797645996094  train KL: 195.80828579101563\n",
      "test CL: 323.21763203125  test KL: 247.9813953125\n",
      "train CL: 321.72524140625  train KL: 228.687337109375\n",
      "test CL: 321.52391171875  test KL: 204.1521765625\n",
      "train CL: 321.54176845703125  train KL: 184.28287431640624\n",
      "test CL: 321.255462109375  test KL: 222.624775390625\n",
      "train CL: 321.4920843261719  train KL: 177.55747739257814\n",
      "test CL: 319.9563859375  test KL: 298.105690625\n",
      "train CL: 320.99786865234375  train KL: 180.38837705078126\n",
      "test CL: 327.9623578125  test KL: 353.0983125\n",
      "train CL: 320.8099323242188  train KL: 199.29931904296876\n",
      "test CL: 320.691626171875  test KL: 189.970923828125\n",
      "train CL: 320.9279908203125  train KL: 181.2990849609375\n",
      "test CL: 320.1630359375  test KL: 240.83818125\n",
      "train CL: 320.52406875  train KL: 159.147459765625\n",
      "test CL: 321.786887890625  test KL: 168.540114453125\n",
      "train CL: 320.26936494140625  train KL: 158.42107392578126\n",
      "test CL: 319.0173634765625  test KL: 161.9568607421875\n",
      "train CL: 320.659483203125  train KL: 152.5612552734375\n",
      "test CL: 318.8738330078125  test KL: 167.3290857421875\n",
      "train CL: 320.79783999023437  train KL: 146.87060473632812\n",
      "test CL: 318.888703125  test KL: 188.957125\n",
      "train CL: 320.06873876953125  train KL: 148.13074873046875\n",
      "test CL: 321.514552734375  test KL: 208.301582421875\n",
      "train CL: 320.13063540039064  train KL: 156.41296049804689\n",
      "test CL: 319.583905078125  test KL: 194.957979296875\n",
      "train CL: 319.7951622558594  train KL: 137.69818676757814\n",
      "test CL: 320.00282421875  test KL: 164.99379140625\n",
      "train CL: 319.5735159667969  train KL: 136.82867934570314\n",
      "test CL: 322.55576796875  test KL: 170.77821796875\n",
      "train CL: 319.75200595703126  train KL: 145.14786943359374\n",
      "test CL: 318.4862705078125  test KL: 148.0879693359375\n",
      "train CL: 319.74151015625  train KL: 147.22766015625\n",
      "test CL: 321.09130625  test KL: 166.1790234375\n",
      "train CL: 319.4833869140625  train KL: 128.1407232421875\n",
      "test CL: 318.7974962890625  test KL: 176.9513912109375\n",
      "train CL: 319.005470703125  train KL: 134.4394849609375\n",
      "test CL: 319.87641640625  test KL: 311.63183359375\n",
      "train CL: 318.5042862304687  train KL: 113.87193876953125\n",
      "test CL: 320.7461681640625  test KL: 137.9097185546875\n",
      "train CL: 319.61554326171876  train KL: 124.85359208984374\n",
      "test CL: 319.7982921875  test KL: 169.64321015625\n",
      "train CL: 318.7449825683594  train KL: 122.42829750976563\n",
      "test CL: 318.2644638671875  test KL: 161.7308103515625\n",
      "train CL: 318.7088721191406  train KL: 120.35719428710938\n",
      "test CL: 317.2188001953125  test KL: 154.0576787109375\n",
      "train CL: 318.5540141845703  train KL: 115.34597663574219\n",
      "test CL: 320.3396509765625  test KL: 136.2724388671875\n",
      "train CL: 318.3472043945313  train KL: 133.15918759765626\n",
      "test CL: 326.774659375  test KL: 143.80883828125\n",
      "train CL: 318.7739370117188  train KL: 111.07361181640626\n",
      "test CL: 316.970382421875  test KL: 217.320944921875\n",
      "train CL: 318.0222581054687  train KL: 127.68523818359375\n",
      "test CL: 317.1410724609375  test KL: 130.1397126953125\n",
      "train CL: 317.6770478515625  train KL: 105.2067849609375\n",
      "test CL: 319.2881765625  test KL: 195.77134296875\n",
      "train CL: 319.01096569824216  train KL: 114.31659445800781\n",
      "test CL: 319.23714921875  test KL: 150.44839140625\n",
      "train CL: 318.39442451171874  train KL: 105.98545712890625\n",
      "test CL: 319.643894140625  test KL: 120.558662109375\n",
      "train CL: 317.68985048828125  train KL: 107.98755029296875\n",
      "test CL: 318.9015470703125  test KL: 125.5693607421875\n",
      "train CL: 317.12192421875  train KL: 112.2624181640625\n",
      "test CL: 316.5645765625  test KL: 138.1146625\n",
      "train CL: 317.4713416748047  train KL: 85.63260910644532\n",
      "test CL: 316.6376806640625  test KL: 117.8280365234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train CL: 317.39893754882814  train KL: 108.87058256835938\n",
      "test CL: 317.1351146484375  test KL: 154.5897400390625\n",
      "train CL: 316.88756840820315  train KL: 88.16607827148438\n",
      "test CL: 318.8325912109375  test KL: 136.1044595703125\n",
      "train CL: 317.42407421875  train KL: 101.75393046875\n",
      "test CL: 315.840619140625  test KL: 282.828018359375\n",
      "train CL: 316.602931640625  train KL: 99.5677369140625\n",
      "test CL: 315.91912734375  test KL: 190.274603125\n",
      "train CL: 317.6676323730469  train KL: 86.41975024414063\n",
      "test CL: 318.426579296875  test KL: 142.376055859375\n",
      "train CL: 316.85592214355466  train KL: 112.35228684082031\n",
      "test CL: 315.4366634765625  test KL: 132.1144537109375\n",
      "train CL: 317.2624266845703  train KL: 87.81812761230469\n",
      "test CL: 316.17753671875  test KL: 168.98747421875\n",
      "train CL: 317.0639587402344  train KL: 94.91612778320312\n",
      "test CL: 316.14378125  test KL: 117.1209578125\n",
      "train CL: 316.4265678955078  train KL: 82.0211715576172\n",
      "test CL: 317.583737109375  test KL: 136.905144140625\n",
      "train CL: 316.1514264892578  train KL: 82.31743151855468\n",
      "test CL: 316.0900919921875  test KL: 106.4717783203125\n",
      "train CL: 316.00040971679687  train KL: 83.44526059570312\n",
      "test CL: 316.0379587890625  test KL: 137.4279560546875\n",
      "train CL: 316.0407133300781  train KL: 80.32046518554688\n",
      "test CL: 315.970812109375  test KL: 107.687476953125\n",
      "train CL: 315.93011071777346  train KL: 91.84551350097657\n",
      "test CL: 315.1003482421875  test KL: 111.5054580078125\n",
      "train CL: 316.47519719238284  train KL: 95.3856830810547\n",
      "test CL: 317.025542578125  test KL: 112.177308203125\n",
      "train CL: 315.6361258544922  train KL: 79.72230070800781\n",
      "test CL: 315.3518109375  test KL: 101.5670765625\n",
      "train CL: 315.6324203857422  train KL: 77.26670324707031\n",
      "test CL: 315.965330078125  test KL: 127.826953515625\n",
      "train CL: 315.52739545898436  train KL: 76.41398422851563\n",
      "test CL: 322.2999474609375  test KL: 117.8082228515625\n",
      "train CL: 316.51388564453123  train KL: 73.75895615234376\n",
      "test CL: 315.3848169921875  test KL: 166.7265767578125\n",
      "train CL: 315.3385650390625  train KL: 70.1730728515625\n",
      "test CL: 315.9248962890625  test KL: 130.4720740234375\n",
      "train CL: 315.5189264892578  train KL: 82.45574772949219\n",
      "test CL: 314.323298828125  test KL: 96.816982421875\n",
      "train CL: 315.3919369873047  train KL: 72.27042805175782\n",
      "test CL: 317.6452939453125  test KL: 99.2873935546875\n",
      "train CL: 315.17785212402345  train KL: 71.81306232910157\n",
      "test CL: 319.4100341796875  test KL: 107.2615853515625\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss,latent_arr,label_arr = train(epoch)\n",
    "    test_loss = test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726c9605",
   "metadata": {},
   "source": [
    "### save latent vector and model\n",
    "\n",
    "save the latent vector for draw the latent image,\n",
    "\n",
    "and save the model for sampling the molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "740aa8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_np = np.array(latent_arr)\n",
    "label_np = np.array(label_arr)\n",
    "\n",
    "latent_np = latent_np[:,:,:2].reshape((-1,2))\n",
    "label_np = label_np[:,:,:2].reshape((-1,2))\n",
    "np.save(\"./result/latent/MW_latent.npy\", latent_np)\n",
    "np.save(\"./result/latent/MW_label.npy\", label_np)\n",
    "\n",
    "torch.save(model.state_dict(), \"./result/model/MW_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
